{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv', 'embeddings', 'test.csv', 'train.csv.zip', 'sample_submission.csv', 'embeddings.zip', 'test.csv.zip', 'sample_submission.csv.zip']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "DATA_PATH = \"../input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, \"test.csv\"))\n",
    "print(\"Train shape : \", train_df.shape)\n",
    "print(\"Test shape : \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "ab2ed2b08c8d3292316ce35ca6f801198aa3cb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22                                                                                                      Has the United States become the largest dictatorship in the world?\n",
      "30                                                                                   Which babies are more sweeter to their parents? Dark skin babies or light skin babies?\n",
      "110                                                                  If blacks support school choice and mandatory sentencing for criminals why don't they vote Republican?\n",
      "114                               I am gay boy and I love my cousin (boy). He is sexy, but I dont know what to do. He is hot, and I want to see his di**. What should I do?\n",
      "115                                                                                                                                    Which races have the smallest penis?\n",
      "119                                                                                                                                       Why do females find penises ugly?\n",
      "127                                                                                             How do I marry an American woman for a Green Card? How much do they charge?\n",
      "144                   Why do Europeans say they're the superior race, when in fact it took them over 2,000 years until mid 19th century to surpass China's largest economy?\n",
      "156                                                                     Did Julius Caesar bring a tyrannosaurus rex on his campaigns to frighten the Celts into submission?\n",
      "167    In what manner has Republican backing of 'states rights' been hypocritical and what ways have they actually restricted the ability of states to make their own laws?\n",
      "Name: question_text, dtype: object\n",
      "0                                                                                                                                   How did Quebec nationalists see their province as a nation in the 1960s?\n",
      "1                                                                                                                          Do you have an adopted dog, how would you encourage people to adopt and not shop?\n",
      "2                                                                                                                                        Why does velocity affect time? Does velocity affect space geometry?\n",
      "3                                                                                                                                                  How did Otto von Guericke used the Magdeburg hemispheres?\n",
      "4                                                                                                                              Can I convert montra helicon D to a mountain bike by just changing the tyres?\n",
      "5                                                                                                                                   Is Gaza slowly becoming Auschwitz, Dachau or Treblinka for Palestinians?\n",
      "6                                                                                          Why does Quora automatically ban conservative opinions when reported, but does not do the same for liberal views?\n",
      "7                                                                                                                                      Is it crazy if I wash or wipe my groceries off? Germs are everywhere.\n",
      "8                                                                                                     Is there such a thing as dressing moderately, and if so, how is that different than dressing modestly?\n",
      "9    Is it just me or have you ever been in this phase wherein you became ignorant to the people you once loved, completely disregarding their feelings/lives so you get to have something go your way an...\n",
      "Name: question_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "insincere_df = train_df.loc[train_df[\"target\"] == 1]\n",
    "sincere_df = train_df.loc[train_df[\"target\"] == 0]\n",
    "pd.set_option(\"max_colwidth\",200)\n",
    "print(insincere_df.head(10)[\"question_text\"])\n",
    "print(sincere_df.head(10)[\"question_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4fcba32677ec2800fc2498c4679f9fc61490398b"
   },
   "outputs": [],
   "source": [
    "# #显示所有列\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# #显示所有行\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# #设置value的显示长度为100，默认为50\n",
    "# pd.set_option('max_colwidth', 400)\n",
    "\n",
    "# # print the longest sentence\n",
    "# sentence_len = train_df[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
    "# # add a col\n",
    "# train_df[\"sentence_len\"] = sentence_len\n",
    "# print(train_df.head(5))\n",
    "# longest_len = np.max(sentence_len)\n",
    "# print(longest_len)\n",
    "# longest_sentence = train_df.loc[train_df[\"sentence_len\"] >= longest_len-70]\n",
    "# # pd.set_option(\"max_colwidth\", 10*int(longest_len))\n",
    "# print(len(longest_sentence))\n",
    "# print(longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "200b43d07d209c6d58b7bc01a5621cce3d63c33c"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# clean data\n",
    "def clean_data(sentence):\n",
    "    # 1. lower\n",
    "    x = str(sentence)\n",
    "    x = x.lower()\n",
    "    \n",
    "    # 2. punct data\n",
    "    puncts = [\n",
    "        ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "        '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    "        '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    "        '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    "        '∙', '）', '↓', '、', '│', '₹', 'π', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    \n",
    "    # 3. number data\n",
    "    x = re.sub('[0-9]{5,}', ' ##### ', x)\n",
    "    x = re.sub('[0-9]{4}', ' #### ', x)\n",
    "    x = re.sub('[0-9]{3}', ' ### ', x)\n",
    "    x = re.sub('[0-9]{2}', ' ## ', x)\n",
    "    \n",
    "    # 4. mispell data\n",
    "    mispell_dict = {\n",
    "        \"aren't\" : \"are not\",\n",
    "        \"can't\" : \"cannot\",\n",
    "        \"couldn't\" : \"could not\",\n",
    "        \"didn't\" : \"did not\",\n",
    "        \"doesn't\" : \"does not\",\n",
    "        \"don't\" : \"do not\",\n",
    "        \"hadn't\" : \"had not\",\n",
    "        \"hasn't\" : \"has not\",\n",
    "        \"haven't\" : \"have not\",\n",
    "        \"he'd\" : \"he would\",\n",
    "        \"he'll\" : \"he will\",\n",
    "        \"he's\" : \"he is\",\n",
    "        \"i'd\" : \"I would\",\n",
    "        \"i'd\" : \"I had\",\n",
    "        \"i'll\" : \"I will\",\n",
    "        \"i'm\" : \"I am\",\n",
    "        \"isn't\" : \"is not\",\n",
    "        \"it's\" : \"it is\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"i've\" : \"I have\",\n",
    "        \"let's\" : \"let us\",\n",
    "        \"mightn't\" : \"might not\",\n",
    "        \"mustn't\" : \"must not\",\n",
    "        \"shan't\" : \"shall not\",\n",
    "        \"she'd\" : \"she would\",\n",
    "        \"she'll\" : \"she will\",\n",
    "        \"she's\" : \"she is\",\n",
    "        \"shouldn't\" : \"should not\",\n",
    "        \"that's\" : \"that is\",\n",
    "        \"there's\" : \"there is\",\n",
    "        \"they'd\" : \"they would\",\n",
    "        \"they'll\" : \"they will\",\n",
    "        \"they're\" : \"they are\",\n",
    "        \"they've\" : \"they have\",\n",
    "        \"we'd\" : \"we would\",\n",
    "        \"we're\" : \"we are\",\n",
    "        \"weren't\" : \"were not\",\n",
    "        \"we've\" : \"we have\",\n",
    "        \"what'll\" : \"what will\",\n",
    "        \"what're\" : \"what are\",\n",
    "        \"what's\" : \"what is\",\n",
    "        \"what've\" : \"what have\",\n",
    "        \"where's\" : \"where is\",\n",
    "        \"who'd\" : \"who would\",\n",
    "        \"who'll\" : \"who will\",\n",
    "        \"who're\" : \"who are\",\n",
    "        \"who's\" : \"who is\",\n",
    "        \"who've\" : \"who have\",\n",
    "        \"won't\" : \"will not\",\n",
    "        \"wouldn't\" : \"would not\",\n",
    "        \"you'd\" : \"you would\",\n",
    "        \"you'll\" : \"you will\",\n",
    "        \"you're\" : \"you are\",\n",
    "        \"you've\" : \"you have\",\n",
    "        \"'re\": \" are\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'll\":\" will\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"tryin'\":\"trying\",\n",
    "\n",
    "        'colour':'color',\n",
    "        'centre':'center',\n",
    "        'didnt':'did not',\n",
    "        'doesnt':'does not',\n",
    "        'isnt':'is not',\n",
    "        'shouldnt':'should not',\n",
    "        'favourite':'favorite',\n",
    "        'travelling':'traveling',\n",
    "        'counselling':'counseling',\n",
    "        'theatre':'theater',\n",
    "        'cancelled':'canceled',\n",
    "        'labour':'labor',\n",
    "        'organisation':'organization',\n",
    "        'wwii':'world war 2',\n",
    "        'citicise':'criticize',\n",
    "        'instagram': 'social medium',\n",
    "        'whatsapp': 'social medium',\n",
    "        'snapchat': 'social medium'\n",
    "    }\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    def replace(match):\n",
    "        return mispell_dict[match.group(0)]\n",
    "    x = mispell_re.sub(replace, x)\n",
    "    \n",
    "    \n",
    "    # 5. stop words\n",
    "    tmp = \"\"\n",
    "    for word in x.split():\n",
    "        if word not in list(STOP_WORDS):\n",
    "            tmp = tmp + word + \" \"\n",
    "    x = tmp\n",
    "    \n",
    "    return x\n",
    "\n",
    "# longest_sentence[\"question_text\"] = longest_sentence[\"question_text\"].apply(lambda x: clean_data(x))\n",
    "# print(longest_sentence[\"question_text\"])\n",
    "# sentence_len = longest_sentence[\"question_text\"].apply(lambda x: len(str(x).split()))\n",
    "# print(sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "f331437af6769d9eb35c34c74e7741dbf4e4fa60"
   },
   "outputs": [],
   "source": [
    "# Loading embedding\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "GLOVE_PATH = os.path.join(DATA_PATH, \"embeddings\", \"glove.840B.300d\", \"glove.840B.300d.txt\")\n",
    "glove_mean = -0.005838499\n",
    "glove_std = 0.48782197\n",
    "GOOGLENEWS_PATH = os.path.join(DATA_PATH, \"embeddings\", \"GoogleNews-vectors-negative300\", \"GoogleNews-vectors-negative300.bin\")\n",
    "PARAGRAM_PATH =  os.path.join(DATA_PATH, \"embeddings\", \"paragram_300_sl999\", \"paragram_300_sl999.txt\")\n",
    "paragram_mean = -0.0053247944\n",
    "paragram_std = 0.49346468\n",
    "WIKI_NEWS = os.path.join(DATA_PATH, \"embeddings\", \"wiki-news-300d-1M\", \"wiki-news-300d-1M.vec\")\n",
    "\n",
    "def load_embedding(file):\n",
    "    def get_coefs(word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    if file == GOOGLENEWS_PATH:\n",
    "        embeddings_index = KeyedVectors.load_word2vec_format(file, binary=True)\n",
    "    elif file == os.path.join(DATA_PATH, \"embeddings\", \"wiki-news-300d-1M\", \"wiki-news-300d-1M.vec\"):\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "ccbce3ef1ba4c0d284648d7d96cf40928f117e26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab_size = 120000\n",
    "\n",
    "def build_vocab(sentences, verbose=True):\n",
    "    # Tokenize the sentences\n",
    "    tokenizer = Tokenizer(\n",
    "        num_words=vocab_size,\n",
    "        filters = '!\"#$%()*+,-./:;<=>?@[\\]^_`{|}~ '\n",
    "    )\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    vocab = tokenizer.word_counts\n",
    "    print(\"word_index_len : \", len(vocab))\n",
    "#     \"\"\"\n",
    "#     :param sentences: list of list of words\n",
    "#     :return: dictionary of words and their count\n",
    "#     \"\"\"\n",
    "#     vocab = {}\n",
    "#     for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "#         for word in sentence.split():\n",
    "#             try:\n",
    "#                 vocab[word] += 1\n",
    "#             except KeyError:\n",
    "#                 vocab[word] = 1\n",
    "#     print(\"word_index_len : \", len(vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "e3062116f958e829bf71f5a457f9fa3899d8433d"
   },
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "2767e44152f651b24948b0ccd242fc0d6559d836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin load glove embedding ...\n",
      "Loading embedding completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin load glove embedding ...\")\n",
    "embeddings_index = load_embedding(GLOVE_PATH)\n",
    "print(\"Loading embedding completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "5b063be035b428c03d26157f53cd05cf1484e9bf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 429/1306122 [00:00<05:04, 4288.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin clean data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [01:32<00:00, 14143.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean data completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin clean data ...\")\n",
    "clean_sentences = train_df[\"question_text\"].progress_apply(\n",
    "    lambda x: clean_data(x))\n",
    "print(\"Clean data completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin build_vocab ...\n",
      "word_index_len :  185187\n",
      "Build vocab completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin build_vocab ...\")\n",
    "clean_vocab = build_vocab(clean_sentences)\n",
    "print(\"Build vocab completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185187/185187 [00:00<00:00, 1201517.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 63.40% of vocab\n",
      "Found embeddings for  98.56% of all text\n",
      "[('quorans', 858), ('brexit', 524), ('cryptocurrencies', 499), ('redmi', 384), ('kvpy', 356), ('paytm', 356), ('iiser', 346), ('ethereum', 334), ('iisc', 278), ('jinping', 211), ('viteee', 186), ('iocl', 179), ('nmims', 163), ('upes', 157), ('rohingya', 157), ('fortnite', 156), ('coinbase', 149), ('nsit', 147), ('cpec', 146), ('iitians', 143)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_oov = check_coverage(clean_vocab, embeddings_index)\n",
    "print(clean_oov[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin load paragram embedding ...\n",
      "Loading embedding completed!\n"
     ]
    }
   ],
   "source": [
    "del embeddings_index\n",
    "gc.collect()\n",
    "print(\"Begin load paragram embedding ...\")\n",
    "embeddings_index = load_embedding(PARAGRAM_PATH)\n",
    "print(\"Loading embedding completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185187/185187 [00:00<00:00, 944253.40it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 74.33% of vocab\n",
      "Found embeddings for  99.16% of all text\n",
      "[('quorans', 858), ('brexit', 524), ('cryptocurrencies', 499), ('redmi', 384), ('coinbase', 149), ('oneplus', 139), ('uceed', 124), ('demonetisation', 115), ('bhakts', 115), ('upwork', 111), ('machedo', 108), ('gdpr', 107), ('adityanath', 106), ('boruto', 102), ('bnbr', 100), ('alshamsi', 92), ('dceu', 90), ('litecoin', 87), ('iiest', 86), ('unacademy', 86)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_oov = check_coverage(clean_vocab, embeddings_index)\n",
    "print(clean_oov[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin load paragram embedding ...\n",
      "Loading embedding completed!\n"
     ]
    }
   ],
   "source": [
    "del embeddings_index\n",
    "gc.collect()\n",
    "print(\"Begin load paragram embedding ...\")\n",
    "embeddings_index = load_embedding(WIKI_NEWS)\n",
    "print(\"Loading embedding completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185187/185187 [00:00<00:00, 1280516.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 48.77% of vocab\n",
      "Found embeddings for  97.19% of all text\n",
      "[('upsc', 2179), ('aiims', 1038), ('cgl', 934), ('quorans', 858), ('jio', 770), ('manipal', 742), ('icse', 659), ('bitsat', 564), ('iiit', 564), ('cgpa', 563), ('ielts', 554), ('mtech', 504), ('ncert', 489), ('clat', 448), ('isro', 445), ('pilani', 390), ('ibps', 389), ('bhu', 386), ('redmi', 384), ('h1b', 380)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clean_oov = check_coverage(clean_vocab, embeddings_index)\n",
    "print(clean_oov[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
